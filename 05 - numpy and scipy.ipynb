{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting The Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The popularity of Python for scientific computing is probably down to its ability to interface to many other compiled languages (C/C++, Fortran, Java, ...). This makes the job of providing a more user friendly interface into hard core number crunching libraries easier.<br><br>\n",
    "Especially the [Cython](https://cython.org/) library has made it easier to write code in a language very close to Python that can be (optimized and) compiled. According to the site: *It makes writing C extensions for Python as easy as Python itself*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scientific Computing Stack](plots/python_sc_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a rough discussion of both NumPy and SciPy. The libraries are vast in scope and this notebook cannot do more than scatch the surface.<br><br>\n",
    "However, to get a grasp of the basics is relatively straightforward. After you get the basics, the rest is fun, and you'll learn it if and when you come across it.<br><br>\n",
    "Good references to learn more & deepen you undestanding are:\n",
    "* Overview of the Python Scientific ecosystem: [Scipy Lecture Notes](https://scipy-lectures.org/)\n",
    "* [NumPy Quickstart Guide](https://numpy.org/devdocs/user/quickstart.html)\n",
    "* [NumPy User quide](https://numpy.org/doc/1.17/user/index.html)\n",
    "* [SciPy Reference](https://docs.scipy.org/doc/scipy/reference/)\n",
    "* [SciPy Tutorial](https://docs.scipy.org/doc/scipy/reference/tutorial/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy: The Foundation For Numerics In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elevator pitch:<br><br>\n",
    "*NumPy (short for Numerical Python) is an open source Python library for doing scientific computing with Python.*<br>\n",
    "*It gives an ability to create multidimensional array objects and perform faster mathematical operations on them.*<br>\n",
    "*The library contains a long list of useful mathematical functions, including some functions for linear algebra and complex mathematical operations such as Fourier Transform (FT) and random number generator (RNG).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importing Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## although not obligatory, everyone assigns the alias np while importing the numpy library:\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Before Starting: An Aside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I just want to focus on functionality to find stuff, starting with the **%psearch** magic.<br>\n",
    "Magic commands exist to make your life more pleasant, see: [Built-in magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html) for more detail.\n",
    "\n",
    "<pre>%psearch [options] pattern [object type]</pre>\n",
    "So, instead of doing **dir(np)** do: **%psearch np.*** <br>\n",
    "Or, if you know the function you are looking for contains the substring matrix, do: \n",
    "**%psearch np.\\*matrix\\***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the result will appear in the popup frame at the bottom of your browser\n",
    "%psearch np.*\n",
    "## this shows the huge functionality provided by NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%psearch np.*matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matrix??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get more help is by using **pydoc**, either using the library or from the commandline. Note that in a Jupyter notebook a system command can be run using the exclamation mark **!command**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pydoc --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pydoc numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that we can search for keywords in the numpy documentation (docstrings) using:<br>\n",
    "**np.lookfor('keyword')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.lookfor('least squares')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The High Level Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Linear Algebra is at the hart of scientific computing.<br>\n",
    "Fast and efficient ways of manipulating multi-dimensional arrays are at the bread and butter of Linear Alebra software.<br>\n",
    "NumPy provides exactly this, and is therefore at the hart of the Python scientific computing stack.<br>\n",
    "NumPy is developed by a huge number of contributors to Python.<br><br>\n",
    "\n",
    "NumPy provides the following:\n",
    "1. An array object of arbitrary homogeneous items\n",
    "2. Fast mathematical operations over arrays\n",
    "3. Linear Algebra, Fourier Transforms, Random Number Generation\n",
    "\n",
    "Almost all libraries that do serious number crunching are build on top of NumPy (use the array datatype and the functions to manipulate them in NumPy).<br>\n",
    "For instance:\n",
    "* SciPy: provides additional algorithms used in scientific computating\n",
    "* Pandas: provide DateFrame and Series objects to work with \n",
    "* SciKit-Learn: provides a wealth of machine learning algorithms accesable using a single clean API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NumPy’s main data type is the homogeneous multidimensional array, **ndarray**.<br>\n",
    "It is a table of elements (usually numbers), all of the **same type**, indexed by a tuple of non-negative integers.<br>\n",
    "In NumPy dimensions are called axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## to create a 1d array you can simply pass in a list\n",
    "lst = []\n",
    "for ix in range(18): lst.append(ix+1)\n",
    "a1d = np.array(lst)\n",
    "print('type =', type(a1d), '& dimension =', a1d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important differences between a numpy array and a list\n",
    "* An array only stores the bits needed to represent the data type\n",
    "* Elements are guaranteed to be located in a single contiguous block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# most of the time it uses less memory\n",
    "import sys\n",
    "print(f'Size of a numpy ndarray   = {sys.getsizeof(a1d)} with the type {a1d.dtype}')\n",
    "print(f'Size of the original list = {sys.getsizeof(lst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the overhead of an ndarray = 96 bytes --> so with the default 8-byte integers we get\n",
    "print(sys.getsizeof(np.array([1])))\n",
    "print(sys.getsizeof(np.array([1,2])))\n",
    "print(sys.getsizeof(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possibly a lot less: np.int8 contains 8 bits = 1 byte has range from -32768 to 32767\n",
    "a1d = a1d.astype(np.int8)\n",
    "print(f'Size with np.int8 = {sys.getsizeof(a1d)} & for the data: {a1d.data.nbytes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to generate matrices in numpy.<br> \n",
    "Figuring out an efficient way to generate a specific array with some complex structure is something you'll get better at through experience.<br><br>\n",
    "Let's start simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## passing a list\n",
    "np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list comprehension\n",
    "np.array([x**2 for x in range(1,11)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one of the many helpre functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a simple 1d sequence\n",
    "np.arange(1, 10.1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## or using linspace\n",
    "np.linspace(1, 10, num=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## passing in a list of lists (as long as each list has the same length)\n",
    "np.array([[1,2,3],[6,5,4],[10,20,30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## an 3x3 array of 0's\n",
    "np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## an 2x6 array of 1's\n",
    "np.ones((2,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a 2x2x2 array of 42's\n",
    "np.full((2,2,2),42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## an 3x3 identity matrix\n",
    "np.identity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a diagonal matrix\n",
    "np.diag([3,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using pseudo random number generation --> randn = standard normal distribution\n",
    "np.random.randn(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## based on some operation of an existing array\n",
    "np.repeat(np.array([1,2,3]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat(np.identity(3), 3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## by stacking existing arrays as passed in as a list\n",
    "np.vstack([np.identity(2),np.identity(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([np.identity(2),3*np.identity(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## moreover, there are many functions that return an array\n",
    "## for instance meshgrid\n",
    "## note: to make it more interesting, let's do a surface plot \n",
    "\n",
    "## some imports needed for plotting (ignore this for now!)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "## generate the x, y, and z coordinates for the surface\n",
    "x = np.arange(-5, 5, 0.01) ## returns an array from -5 to 5 with steps of length 0.1\n",
    "y = np.arange(-5, 5, 0.01)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "z = np.sin((xx/2)**2 + (yy/2)**2)\n",
    "\n",
    "## and plot it\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(xx, yy, z, antialiased=False, cmap=cm.coolwarm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy **ndarray** is a rich data type, with many methods.<br>\n",
    "The most important ones are related to manipulating the shape and the memory of the array.<br><br>\n",
    "As an aside, although there are some 'mathematical' functions defined as methods (like: min, max, cumsum, ...), the vast majority of math functions are defined as functions, aka universal functions, where the array is passed in as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## have a look at what is defined\n",
    "%psearch np.ndarray.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what is the shape of the array\n",
    "arr = np.array([[1,2,3,4],[5,4,3,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can also get to the low level data (ndarray.data returns a memory view object)\n",
    "## so all the bits in the data as a string in hex format\n",
    "arr.data.hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seems data is stored in 'little endian' notation\n",
    "arr.data.hex()[16:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr.resize((3,6))\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Operations & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## operations are applied to the complete array\n",
    "arr < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if the return value is an array -> additional methods can be applied to the result\n",
    "## which row has all element < 10\n",
    "(arr<10).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many rows with all elements < 10 (summing over booleans -> True = 1 & False = 0)\n",
    "(arr<10).all(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(arr<10).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the methods are applied to the complete array\n",
    "print('array:', a1d)\n",
    "print('sum:', a1d.sum())\n",
    "print('max:', a1d.max())\n",
    "print('argmax:', a1d.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## these method are often slightly faster than the equevalent functions passing in an array\n",
    "%timeit a1d.argmax()\n",
    "%timeit np.argmax(a1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are many many more familiar mathematical functions defined in the NumPy namespace, such as sin, cos, exp, .... These are called **universal functions, or ufunc** and operate elementwise on an array, producing an array as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.lookfor('ufunc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to equivalend operations on lists or tuples, numpy arrays are often a lot quicker.<br>\n",
    "Lets shuffle the array / list using a random permutation of the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first generate a random permutation of the sequence 0 .. length(a1d / lst)\n",
    "random_index = np.random.permutation(np.arange(len(lst)))\n",
    "random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now lets use the %timeit magic command to:\n",
    "## time the reordering of the list using the random permutation\n",
    "%timeit lst_reordered = [lst[i] for i in random_index]\n",
    "## time the reordering of the array using the random permutation\n",
    "%timeit a1d_reordered = a1d[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## advanced: this speedup is only fully realised when we do as much work as possible in each call\n",
    "## below we jump into the compiled code 10.000 times --> now the list is actually faster\n",
    "%timeit for i in random_index: lst[i]\n",
    "%timeit for i in random_index: a1d[i]\n",
    "## if you want to implement an algorithm using this type of pattern please look into Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## most operation on an array are 'vectorized' meaning it will automatically be executed for each element\n",
    "## so, to generate an array that: (1) take modulo 2 (using the modulo % operator) & (2) checks if the result = 0\n",
    "(a1d % 2) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from being faster, the code is often much more readable once you get used to it.<br>\n",
    "As an example, lets compute the z-score of *lst / a1d*:\n",
    "$$ z = (x - \\mu_x) / \\sigma_x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "## need to make the data a bit bulkier to see the speed improvement\n",
    "lst = list(range(100000))\n",
    "a1d = np.array(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define function to compute the z-score for a list\n",
    "def compute_z_score(a_list):\n",
    "    x_avg = sum(a_list)/len(lst)\n",
    "    x_var = sum([(x - x_avg)**2 for x in a_list]) / (len(a_list)-1)\n",
    "    x_sd  = math.sqrt(x_var)\n",
    "    z     = [(x-x_avg)/x_sd for x in a_list]\n",
    "    return(z)\n",
    "## call the function and time it\n",
    "%timeit z = compute_z_score(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z[:5], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit z = (a1d - a1d.mean()) / a1d.std()\n",
    "print(z[:5])\n",
    "%timeit z = (a1d - a1d.mean()) / a1d.std(ddof=1)\n",
    "print(z[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## to create a 2d array, do:\n",
    "a2d = np.array([[2, 0], [0, 1]])\n",
    "a2d + a2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2d arrays are basically matrices, to do matrix multiplication use @ or np.dot\n",
    "print(a2d @ a2d)\n",
    "print(np.dot(a2d,a2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## to create a 3d array\n",
    "a3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
    "print(a3d, '\\n', '-'*100)\n",
    "\n",
    "s2 = np.identity(2) * 2\n",
    "print(a3d @ s2)\n",
    "print('\\n', '-'*100)\n",
    "\n",
    "## an ndarray can be reshaped, as long as the number of elements remain the same\n",
    "a3d = a3d.reshape([1,2,1,6])\n",
    "print(a3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operation above were done on arrays of different shape.<br>\n",
    "The rules for dealing with this mismatch in dimension are called **broadcasting**.\n",
    "<br><br>\n",
    "<i>\n",
    "Te term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes.\n",
    "</i>\n",
    "<br><br>\n",
    "See: [basic broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).<br>\n",
    "In most simple case this **broadcasting** of dimensions is very intuitive, but it can fast get really tricky in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.array([1,2,3,4,5]) * 2 + np.array([[1,1,1,1,1],[2,2,2,2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## broadcasting rules do not allow an operation that do not make sense, for instance: two 1d arrays of unequal length\n",
    "try:\n",
    "    np.array([1,2,3,4,5]) + np.array([10,20])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "## but ... they do kick in implicitly when the rules allo for it! Mostly, this is completely intuitive (not always!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## broadcasting rules do allow operation with different dimensions, as long as it can *broadcast*\n",
    "## the 'smaller' array into the 'bigger' one\n",
    "np.array([[1, 2],[3, 4]]) + np.array([10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NumPy supports a much greater variety of numerical types than Python does. The following table shows different scalar data types defined in NumPy.\n",
    "\n",
    "| Sr.No. | Data Types & Description\n",
    "| :----- | :-----\n",
    "| bool_ | Boolean (True or False) stored as a byte\n",
    "| int_ | Default integer type (same as C long; normally either int64 or int32)\n",
    "| intc | Identical to C int (normally int32 or int64)\n",
    "| intp | Integer used for indexing (same as C ssize_t; normally either int32 or int64)\n",
    "| int8 | Byte (-128 to 127)\n",
    "| int16 | Integer (-32768 to 32767)\n",
    "| int32 | Integer (-2147483648 to 2147483647)\n",
    "| int64 | Integer (-9223372036854775808 to 9223372036854775807)\n",
    "| uint8 | Unsigned integer (0 to 255)\n",
    "| uint16 | Unsigned integer (0 to 65535)\n",
    "| uint32 | Unsigned integer (0 to 4294967295)\n",
    "| uint64 | Unsigned integer (0 to 18446744073709551615)\n",
    "| float_ | Shorthand for float64\n",
    "| float16 | Half precision float: sign bit, 5 bits exponent, 10 bits mantissa\n",
    "| float32 | Single precision float: sign bit, 8 bits exponent, 23 bits mantissa\n",
    "| float64 | Double precision float: sign bit, 11 bits exponent, 52 bits mantissa\n",
    "| complex_ | Shorthand for complex128\n",
    "| complex64 | Complex number, represented by two 32-bit floats (real and imaginary components)\n",
    "| complex128 | Complex number, represented by two 64-bit floats (real and imaginary components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "arr_cmplx = np.array([1,2,3,4,5],dtype=np.complex)\n",
    "arr_cmplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr_float16 = arr_cmplx.astype(np.float16)\n",
    "arr_float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## array indexing works pretty much like it does with lists\n",
    "arr_float16[2:4] += 10\n",
    "arr_float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Missing in NumPy: Missing Integer Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## foating point numbers have a missing value in NumPy\n",
    "arr_float16[4] = np.NAN\n",
    "arr_float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## integer data types do not have a missing value / NaN in NumPy, this is problematic\n",
    "## as a consequence all libraries build on top of NumPy lack native support for integer missing values\n",
    "## you have to (1) convert to float, or (2) use a masked array\n",
    "## worse yet, when converting from floating point to integer value, missing values become 0's\n",
    "arr_int16 = arr_float16.astype(np.int16)\n",
    "arr_int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    arr_int16[4] = np.NaN\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is just no way we can set an element of a normal numpy array of some integer dtype to missing!<br>\n",
    "Masked arrays are arrays that may have missing or invalid entries.<br>\n",
    "The numpy.ma module provides a nearly work-alike replacement for numpy that supports data arrays with masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## lets say we have array, where 99 is used as missing value\n",
    "ar = [1,2,99,4,5]\n",
    "## and we want to mask the missing values\n",
    "ma = np.ma.masked_equal(ar, 99)\n",
    "ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## for the sum we use\n",
    "ma.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for the dot product we use:\n",
    "np.ma.dot(ma,np.ones(ma.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.sum(ma*np.ones(ma.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Stacking & Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## stacking\n",
    "print(np.hstack([np.identity(2),np.identity(2)]))\n",
    "print('-'*20)\n",
    "print(np.vstack([np.identity(2),np.identity(2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.array([1,2,3,4,5,6]).reshape((2,-1))\n",
    "## this is the same as order='C' --> first loop over the 'right-most' index (for 2D array columns)\n",
    "np.array([1,2,3,4,5,6]).reshape((2,-1),order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## order = 'F' uses the Fortran rule: first loop over 'left' (for 2D array rows) indices first\n",
    "np.array([1,2,3,4,5,6]).reshape((2,-1),order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## the shapes of the vectors line up\n",
    "v1 = np.array([1,2,3])\n",
    "v2 = np.array([4,5,6])\n",
    "v1 + v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## what happens when we add a column vector and a row vector\n",
    "## linear algebra does not define this\n",
    "## --> numpy broadcasts the shapes into something that is valid and defined mathematicallly\n",
    "v1 = np.array([ 1, 2, 3]).reshape((-1, 1))\n",
    "v2 = np.array([10,20,30]).reshape(( 1,-1))\n",
    "v1 + v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Above the column vector gets 'expanded' into a matrix where the columns are replicates of the column vector & (replicated # elements in the row vector)<br>\n",
    "row vector gets 'expanded' into a matrix where the rows are replicates of the row vector (replicated # elements in the column vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Broadcasting in NumPy follows a strict set of rules to determine the interaction between the two arrays:\n",
    "* **Rule 1**: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side.\n",
    "* **Rule 2**: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.\n",
    "* **Rule 3**: If in any dimension the sizes disagree and neither is equal to 1, an error is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lets work through an example\n",
    "M = np.ones((2, 3))\n",
    "a = np.arange(3)\n",
    "M * a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "shape M = (2, 3) & shape a = (3,)<br>\n",
    "**rule 1** pad a to the left with ones<br>\n",
    "shape M = (2, 3) & shape a = (1,3) ==> a= [[0,1,2]]<br>\n",
    "**rule 2** first dim disagrees and has value 1 in a, so we 'stretch' dim 1 of a to 2 (replicating each element)<br>\n",
    "shape M = (2, 3) & shape a = (2,3) ==> a= [[0,1,2],[0,1,2]]<br>\n",
    "We are now good to go!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "M = np.ones((3, 2))\n",
    "a = np.arange(3)\n",
    "M * a ## a will become (1) [[0,1,2]] (2) [[0,1,2],[0,1,2],[0,1,2]] --> but (3,2) * (3,3) is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### np.newaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## if we want to be explicit and replicate the vector into a matrix with columns [a,a], we need to be explicit where we want the new expanded axis to go using np.newaxis\n",
    "M * a[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions On Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## numpy has many functions defined that do the right thing on array's\n",
    "x = np.linspace(0,2*np.pi,num=5)\n",
    "np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## what is the index where the max occurs\n",
    "np.argmax(np.sin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "2 * np.random.random(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Let's implement least squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1,10,20)\n",
    "y = 12 + 0.5 * x + 1 * np.random.rand(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If $y^* = a + b * x$ and let $\\beta = [a,b]$, then the least squares estimate boils down to:<br>\n",
    "minimize $(y - y^*)^T (y - y^*) = (y - (\\beta_1 + \\beta_2 x))^T (y - \\beta_1 + \\beta_2 x)$ by setting the derivative to $\\beta$ to $0$, resulting in:<br>\n",
    "$(X^T X)\\beta = X^T y$ or $\\beta = (X^T X)^-1 (X^T y)$<br>\n",
    "This is most efficiently solved using **np.linalg.solve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def least_squares_reg(x, y):\n",
    "    ## add column of ones for the intercept\n",
    "    X = np.vstack([np.ones(len(x)),x]).T\n",
    "    b = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    return(b)\n",
    "least_squares_reg(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Could also use the scikit-learn library\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "lm  = reg.fit(x.reshape(-1,1), y)\n",
    "print(f'intercept = {lm.intercept_}')\n",
    "print(f'slope = {lm.coef_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Numbers and Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a lot of cool functionality for random sampling is provided in numpy.random\n",
    "dir(np.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to generate a bunch of pseudo random numbers from the unifor distribution on [0.0,1.0), use:\n",
    "np.random.rand(3,5) ## 3 rows & 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random integer\n",
    "np.random.randint(low=1, high=10, size=(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random numbers from the F-distribution (with df numerator = 101 and df denominator = 35\n",
    "np.random.f(101,35,size=(2,5)) ## 2 rows & 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(['A','B','C','D'], size=(3,7), replace=True, p=[0.1,0.1,0.1,0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffling\n",
    "lst = [1,2,3,4,5,6,7,8,9]\n",
    "np.random.shuffle(lst)\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement a nonparametric test for group means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = np.random.binomial(20,0.48,size= 5);\n",
    "g2 = np.random.binomial(20,0.52,size=10);\n",
    "print(f'mean group 1 {np.mean(g1)} & mean group 2 {np.mean(g2)} --> difference = {np.mean(g2) - np.mean(g1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_same = np.concatenate([g1,g2])\n",
    "def mean_dif(df_same):\n",
    "    np.random.shuffle(df_same)\n",
    "    return(np.mean(df_same[:5]) - np.mean(df_same[5:]))\n",
    "\n",
    "dist_mean_diff = [mean_dif(df_same) for _ in range(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dist_mean_diff, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'% where diff in mean between the two groups is > 1.3 = {np.sum(np.array(dist_mean_diff) > 1.3) / len(dist_mean_diff)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy provides a mechanism to vectorize an ordinary Python function which accepts scalars and returns scalars into a “vectorized-function” with the same broadcasting rules as other NumPy functions (i.e. the Universal functions, or ufuncs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## say we have a function\n",
    "def addsubtract(a,b):\n",
    "    if a > b:\n",
    "        return a - b\n",
    "    else:\n",
    "        return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsubtract(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as-is this function is not 'vectorized' it doesn't work on arrays (doesn't broadcast)\n",
    "try:\n",
    "    addsubtract(np.array([5,2,-1]),3)\n",
    "except Exception as err:\n",
    "    print(f'Error: {err}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the np.vectorize does give the vectorized version of the function\n",
    "addsubtract_vectorized = np.vectorize(addsubtract)\n",
    "addsubtract_vectorized(np.array([5,2,-1]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more tricks and shortcuts. To become good, you need to use the library. For now, you know much there is to know ... enough to tackle most problems with a little help from the documentation and google. Have fun exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy: Scientific Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy builds on NumPy, and for all basic array handling needs you can use NumPy functions.<br>\n",
    "The top level of scipy also contains functions from numpy and numpy.lib.scimath. However, it is better to use them directly from the numpy module instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%psearch sp.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **import scipy as sp** gives us almost exactly what **import numpy as np** gives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The angle of the complex number {1+1j} using scipy.angle = {sp.angle(1+1j)} which is {sp.angle(1+1j)/sp.pi} x pi')\n",
    "print(f'The angle of the complex number {1+1j} using numpy.angle = {np.angle(1+1j)} which is {np.angle(1+1j)/np.pi} x pi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at what is defined in scipy and not in numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## np.setdiff1d(a1, a2) returns the elements in a1 that are not in a2\n",
    "np.setdiff1d( np.array([fn for fn in dir(sp) if not(fn.startswith('__'))]),\n",
    "              np.array([fn for fn in dir(np) if not(fn.startswith('__'))])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as an aside it is intersting to see which version of blas / lapack are configured\n",
    "## mkl or libopenblas are highly optimized -- or, even better you'd use cublas (cuda blas - nvidia gpu implementation)\n",
    "## for scientific computation having an optimized blas is critical (having a reference implementation can be order of magnitude slower)\n",
    "sp.show_numpy_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra functionality of SciPy is implemented in it's **modules**. These need to be imported explicitly:\n",
    "<pre>\n",
    ">>> from scipy import some_module\n",
    ">>> some_module.some_function()\n",
    "</pre>\n",
    "Let's have a look at the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, below are the main topics in the SciPy tutorials with worked examples and background information:<br><br>\n",
    "[SciPy Tutorial](https://docs.scipy.org/doc/scipy/reference/tutorial/index.html):\n",
    "* [Introduction](https://docs.scipy.org/doc/scipy/reference/tutorial/general.html)\n",
    "* [Basic functions](https://docs.scipy.org/doc/scipy/reference/tutorial/basic.html)\n",
    "* **scipy.special** [Special functions](https://docs.scipy.org/doc/scipy/reference/tutorial/special.html)\n",
    "* **scipy.integrate** [Integration](https://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html)\n",
    "* **scipy.optimize** [Optimization](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)\n",
    "* **scipy.interpolate** [Interpolation](https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html)\n",
    "* **scipy.fftpack** [Fourier Transforms](https://docs.scipy.org/doc/scipy/reference/tutorial/fftpack.html)\n",
    "* **scipy.signal** [Signal Processing](https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html)\n",
    "* **scipy.linalg** [Linear Algebra](https://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html)\n",
    "* [Sparse Eigenvalue Problems with ARPACK](https://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html)\n",
    "* **scipy.sparse.csgraph** [Compressed Sparse Graph Routines](https://docs.scipy.org/doc/scipy/reference/tutorial/csgraph.html)\n",
    "* **scipy.spatial** [Spatial data structures and algorithms](https://docs.scipy.org/doc/scipy/reference/tutorial/spatial.html)\n",
    "* **scipy.stats** [Statistics]()\n",
    "* **scipy.ndimage** [Multidimensional image processing](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html)\n",
    "* **scipy.io** [File IO](https://docs.scipy.org/doc/scipy/reference/tutorial/io.html)\n",
    "\n",
    "The depth of topis covered in the above goes well beyond the scope of this notebook, and well beyond the sope of my knowledge. Again, what is important is roughly knowing what is there. Below are a couple of examples that scratch the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%psearch ss.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## binomial coefficient: the number of ways, disregarding order, that k objects can be chosen from among n objects\n",
    "## scipy gives you two options: binom & comb --> where comb with exact=True uses Python integers to get the exact result\n",
    "ss.binom(5,3) == ss.comb(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ss.binom(75,15)\n",
    "ss.binom(75,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ss.comb(75,15, exact=True)\n",
    "ss.comb(75,15, exact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Binomial Distribution function:\n",
    "$$\\Pr(k;n,p)=\\Pr(X=k)={\\binom {n}{k}}p^{k}(1-p)^{n-k}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the binomial coefficient as above --> the probability mass function for the binomial\n",
    "binom_pmf = np.vectorize(lambda k, n, p: ss.binom(n, k) * p**k * (1-p)**(n-k))\n",
    "## note: vectorize converts the function to a vectorized version (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets compute the probability of exactly k successes in 50 throws with pr success 20%\n",
    "n = 50\n",
    "p = 0.2\n",
    "nr_successes = np.arange(n+1)\n",
    "pr_nr_successes = binom_pmf(nr_successes, n, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets also create some plots to get a better feel\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## so the binom(k; n=50, p=0.2) looks like:\n",
    "plt.subplots(nrows=1, ncols=1, figsize=(10,3))\n",
    "plt.bar(nr_successes, pr_nr_successes);\n",
    "plt.title(f'Binomial PMF (probability mass function) for k successes with\\nN = {n} trials & probability of success p = {p:.1f}');\n",
    "plt.xlabel('# successes k');\n",
    "plt.ylabel('probability of k successes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for the cumulative mass function:\n",
    "pr_nr_successes_le_k = np.cumsum(pr_nr_successes)\n",
    "## and plot it\n",
    "plt.subplots(nrows=1, ncols=1, figsize=(10,3))\n",
    "plt.bar(nr_successes, pr_nr_successes_le_k);\n",
    "plt.title(f'Binomial CMF (cumulative mass function) for k or less successes with\\nN = {n} trials & probability of success p = {p:.1f}');\n",
    "plt.xlabel('k or less successes');\n",
    "plt.ylabel('probability of k or less successes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apart from defining the pmf of the binomial and doing a np.cumsum\n",
    "## scipy.special has a more optimized direct way to compute this: ss.bdtr(k, n, p)\n",
    "plt.subplots(nrows=1, ncols=1, figsize=(10,3))\n",
    "plt.bar(nr_successes, ss.bdtr(nr_successes, n, p));\n",
    "plt.title(f'Binomial CMF (using scipy.special.bdtr)\\nN = {n} trials & probability of success p = {p:.1f}');\n",
    "plt.xlabel('k or less successes');\n",
    "plt.ylabel('probability of k or less successes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean, or the expected value, for binom(k; 50, 0.2) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## expected value: sum of k * pr(k)\n",
    "np.sum(nr_successes * pr_nr_successes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scipy also gives objects representing satatistical distributions\n",
    "## for instance for the binomial distribution:\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can ask what the theoretical mean is of a binom(k; n=50, p=0.2)\n",
    "## statistics 101: n x p\n",
    "binom(n=50,p=0.2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## or the variance is of a binom(k; n=50, p=0.2)\n",
    "## statistics 101: n x p x (1-p)\n",
    "binom(n=50,p=0.2).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r if we feel like computing it ourselves\n",
    "$$\\mu_{2}=\\int _{-\\infty }^{\\infty }(x-\\mu_{1})^{2}\\,f(x)\\,\\mathrm {d} x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = np.sum(nr_successes * pr_nr_successes)\n",
    "np.sum((nr_successes - u1)**2 * pr_nr_successes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## numeric integration using:\n",
    "## trapezoidal rule\n",
    "## simpsons rule\n",
    "## gaussian quadrature\n",
    "from scipy.integrate import trapz, simps, quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets say we have a function: x**3 - 2 * x**2 + 3 * x + 1\n",
    "## def fnc_non_vectorized(x): return(x**3 - 2 * x**2 + 3 * x + 1)\n",
    "## f2i = np.vectorize(fnc_non_vectorized)\n",
    "f2i = np.vectorize(lambda x: x**3 - 2 * x**2 + 3 * x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\int{x^3 - 2 x^2 + 3 x + 1} dx = \\frac{x^4}{4} - \\frac{2}{3} x^3 + \\frac{3}{2} x^2 + x + constant$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## so the analytical solution:\n",
    "def fint(a,b):\n",
    "    fun = lambda x: (1/4) * x**4 - (2/3) * x**3 + (3/2) * x**2 + x\n",
    "    return(fun(a)-fun(b))\n",
    "fint(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,num=20)\n",
    "y = f2i(x)\n",
    "plt.plot(x,y,'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trapz(y,x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each 2 consecutive points, the area of the trapezoidal = delta_x * avg_y\n",
    "my_trapz = (x[1:]-x[:-1]) * (y[:-1]+y[1:]) / 2\n",
    "np.sum(my_trapz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision 2 \n",
    "np.vstack([  y[:-1],                         ## y_0, ..., y_(n-1)\n",
    "             y[1:],                          ## y_1, ..., y_n\n",
    "             y[:-1]+y[1:]/2,                 ## avg height\n",
    "             x[1:]-x[:-1],                   ## delta x \n",
    "             (x[1:]-x[:-1])*(y[:-1]+y[1:])/2 ## area trapezium\n",
    "          ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simps(y,x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad(f2i, a=-10, b=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scipy.optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## minimize function of one argument\n",
    "help(scipy.optimize.minimize_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x): return((x-5)**2+7)\n",
    "print(minimize_scalar(fun))\n",
    "x = np.linspace(-10,10,num=100)\n",
    "plt.plot(x, np.apply_along_axis(fun,0,x),'-r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x): return(-(x-5)**2+7)\n",
    "try:\n",
    "    opt_result = minimize_scalar(fun)\n",
    "except OverflowError as e:\n",
    "    print(e)\n",
    "x = np.linspace(-10,10,num=100)\n",
    "plt.plot(x, np.apply_along_axis(fun,0,x),'-r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## but watch out for local minima ...\n",
    "def fun(x): return(x**3 - 5*x**2 + 1)\n",
    "print(minimize_scalar(fun))\n",
    "x = np.linspace(-2, 5, num=100)\n",
    "plt.plot(x, fun(x), '-r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate optimization is a *huge* topic, there are many techniques available. Which method is efficient / usable, depends on many properties of the problem at hand: (1) form of the objective function (2) do we have 1st and/or 2nd order derivatives (3) do we have contraints, ...<br>\n",
    "\n",
    "A few remarks to try and lift a tip of the complexity:\n",
    "* Unconstrained Problems:\n",
    "  * Nelder–Mead method (aka downhill simplex method): is a direct search method (based on\n",
    "    function comparison) and is often applied to nonlinear optimization problems for which \n",
    "    derivatives may not be known. \n",
    "  * BFGS method belongs to quasi-Newton methods: a class of hill-climbing optimization\n",
    "    techniques that seek a stationary point of a (preferably twice continuously \n",
    "    differentiable) function. It is based on the first terms of the Taylor expansion. In \n",
    "    Quasi-Newton methods, the Hessian matrix of second derivatives is not computed but\n",
    "    approximated using updates based on gradient evaluations (approx. gradient evaluations).\n",
    "* Constrained Problems:\n",
    "  * Linear Programming: $\\text{minimize }\\mathbf{c}^{\\mathrm{T}}\\mathbf{x}$ \n",
    "    $\\text{subject to }A\\mathbf{x}\\preceq\\mathbf{b}$ -->  optimization of a linear objective\n",
    "    function, subject to linear equality and linear inequality constraints.\n",
    "  * Quadratic Programming: $\\text{minimize }\\tfrac{1}{2}\\mathbf{x}^{\\mathrm{T}}Q\\mathbf{x}+\n",
    "    \\mathbf{c}^{\\mathrm{T}}\\mathbf{x}$ $\\text{subject to }A\\mathbf{x}\\preceq\\mathbf{b}$ -->\n",
    "    optimizing (minimizing or maximizing) a quadratic function of several variables subject\n",
    "    to linear constraints on these variables. The scipy.optimize.minimize(method=’SLSQP’)\n",
    "    solver implements a SQP (sequential quadratic programming) algorithm that solves a\n",
    "    sequence of optimization subproblems, each of which optimizes a quadratic model of the\n",
    "    objective subject to a linearization of the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scipy.optimize.minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS an example, let's minimize a multivariate nonlinear function with inequality conditions & equality conditions and bounds:\n",
    "<pre>\n",
    "objective: \n",
    "    minimize x0 * x3 * (x0 + x1 + x2) + x3\n",
    "subject to:\n",
    "    1) inequality contidion 1: x0 * x1 * x2 * x3 >= 25\n",
    "    2) equality contidion 2: x0**2 + x1**2 + x2**2 + x3**2 = 40\n",
    "    3) range condition: 1 <= x0, x1, x2, x3 <= 5\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define function to capture the objective\n",
    "def objective(x):              return(x[0] * x[3] * (x[0] + x[1] + x[2]) + x[3])\n",
    "## define functions to capture (in)equality conditions\n",
    "def inequality_constraint(x):  return(np.prod(x) - 25)           ## form: f(x) >= 0\n",
    "def equality_constraint(x):    return(np.sum(np.square(x)) - 40) ## form: f(x)  = 0\n",
    "## put bounds on individual parameters\n",
    "param_bounds = ((1,5),(1,5),(1,5),(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = minimize(objective,                      ## the objective function\n",
    "               np.array([1.0, 5.0, 5.0, 1.0]), ## starting value\n",
    "               method = 'SLSQP',               ## S(equential) L(east) SQ(uares) P(rogramming)\n",
    "               constraints = ({'type':'ineq', 'fun':inequality_constraint},\n",
    "                              {'type':'eq',   'fun':equality_constraint}\n",
    "                             ),\n",
    "               bounds = param_bounds\n",
    "              )\n",
    "sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%psearch scipy.linalg.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy.linalg has methods for most matrix decompositions: Cholesky, LU, QR, Eigen, SVD.<br>\n",
    "SciPy.linalg has methods to solve systems of linear equations.<br>\n",
    "SciPy gives access to the low level routines in BLAS and LAPACK.<br>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA is computed via eigen decomposition of the covariance matrix C.\n",
    "x1 = np.random.randn(250)\n",
    "x2 = np.random.randn(250) ## x1 & x2 independent\n",
    "x3 = 0.8 * (x1 + x2) / 2 + 0.2 * np.random.randn(250) ## x3 depends on x1 & x2\n",
    "x4 = 0.7 * (x1 + x3) / 2 + 0.3 * np.random.randn(250) ## x4 depends on x1 & x2 & x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmat = np.vstack([x1,x2,x3,x4]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(x):\n",
    "    ## compute the covariance matrix: np.dot(X.T, X) / (x.shape[0]-1)\n",
    "    C = np.cov(x, rowvar=False, bias=False)\n",
    "    ## compute the eigen decomposition of the covariance matrix\n",
    "    evals, evecs = np.linalg.eig(C)\n",
    "    ## return the principal components: X . <<matrix where columns are the eigen vectors>>\n",
    "    return(np.dot(x, evecs), evecs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_my_pca, x_my_pca_params = my_pca(xmat)\n",
    "x_my_pca_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pca using scikit learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(xmat)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the goal here is to show the working in a more low level lib, giving you much more power\n",
    "## of course scikit-learn will give you much ritcher information, for instance:\n",
    "## how many variables were really underlying our toy dataset\n",
    "ev = 100 * pca.explained_variance_ / pca.explained_variance_.sum()\n",
    "cev = np.cumsum(ev)\n",
    "print('explained variance:', *[f'pca{ix+1}= {ev:.2f}%' for ix,ev in enumerate(ev)])\n",
    "print('cumulative:', *[f'pca{ix+1}= {cev:.2f}%' for ix,cev in enumerate(cev)])\n",
    "## the pca tells us that if we reduce the data from 4 columns to 2 we retain 96% of the information\n",
    "## ... we only really used two independent variables to create the 4 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Level Access To BLAS & LAPACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import blas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(blas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the performance or memory behaviour of linear algebra on large arrays can suffer because of copying behind the scenes. Let's compute $A^T.A$ for a large matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t = time(); \n",
    "\n",
    "N = 5*int(1e6)\n",
    "n = 40\n",
    "A = np.ones((N,n))\n",
    "C = np.dot(A.T, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.dot(A.T,  A);\n",
    "time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time(); \n",
    "## there are a couple of things to note here\n",
    "## 1. A is a standard 'C' order --> meaning A.T is in 'F' order without touching the matrix\n",
    "## 2. dsyrk computes: alpha.A.A' so then passing in A.T we get: alpha.(A.T).(A.T.T) = alpha.(A.T).A\n",
    "C2 = blas.dsyrk(alpha=1.0, a=A.T);\n",
    "## 2. dsyrk returns the upper triangular part -> to get back to the full symmetric result:\n",
    "C2 = C2 + C2.T - np.diag(np.diagonal(C2))\n",
    "time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T.flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to np.dot(A.T, A) creates a copy of A. When A is large this takes up memmory<br>\n",
    "The low level call to BLAS does not create a copy, and is therfore more efficient.<br><br>\n",
    "Please note that tis is an **advanced topic** and definitely not a route you should choose in first instance ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats.distributions as statdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a rich collection of statistical distribution defined in scipy.<br>\n",
    "The interface these distributions provide is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| method | description |\n",
    "| --- | --- |\n",
    "| rvs | Random Variates |\n",
    "| pdf or pmf| Probability Density/Mass Function |\n",
    "| cdf | Cumulative Distribution Function |\n",
    "| sf | Survival Function (1-CDF) |\n",
    "| ppf | Percent Point Function (Inverse of CDF) |\n",
    "| isf | Inverse Survival Function (Inverse of SF) |\n",
    "| stats | Return mean, variance, (Fisher’s) skew, or (Fisher’s) kurtosis |\n",
    "| moment | non-central moments of the distribution |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a distribution object, for instance a Bernouilli distribution with p=0.5\n",
    "bd = statdist.bernoulli(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a sample of 10 draws from the \n",
    "bd.rvs(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.pmf(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cumulative <=1 \n",
    "bd.cdf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## expected value ... is equal to p\n",
    "bd.expect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variance ... p*(1-p)\n",
    "bd.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of $n$ iid Bernouilli($p$) variables is distributed as Binomial($n$,$p$).<br>\n",
    "$$ Pr(k=\\textstyle{\\sum}_{i=1}^{n}x_i;p)=\\prod_{i=1}^{n} p^{x_i} (1 - p^{1-x_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom = statdist.binom(n=10, p=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(15,5))\n",
    "k = np.arange(11)\n",
    "plt.bar(k,binom.pmf(k));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a population where the probability of being female is $p$.<br>\n",
    "The goal is to estimate this probability given a sample of size $N$.<br>\n",
    "The sample $\\textbf{x} = x_1, x_2, ..., x_N$ (with $x_i = 1$ for female and $x_i = 0$ for male) has pmf Binomial(n,p):\n",
    "$$ Pr(k=\\textstyle{\\sum}_{i=1}^{n}x_i;p)=\\prod_{i=1}^{N} p^{x_i} (1 - p^{1-x_i})$$\n",
    "This same function, but now with the data fixed as function of the $p$ is called the likelihood function:\n",
    "$$ Pr(p;\\textbf{x})=\\prod_{i=1}^{N} p^{x_i} (1 - p^{1-x_i})$$\n",
    "The value of $p$ where this function has a maximum $\\hat{p}$ is called the **maximum likelood estimator of $p$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lh_binom(p, x): return(np.product(np.hstack((p**x,(1-p)**(1-x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a random sample of size $100$ of Bernouilli(0.488)\n",
    "sample = statdist.bernoulli(0.488).rvs(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(15,5))\n",
    "## note 1: need to call .reshape((-1,1)) to convert into column vector\n",
    "p = np.linspace(0,1,num=100).reshape((-1,1))\n",
    "## note 2: cannot call lh_binom(p, sample) --> which would pass two arrays into the function\n",
    "## this is not what we want, we want to call lh_binom(p_i, sample) for each of the 100 elements of p\n",
    "## use apply_along_axis !!! with axis=1 we say call lh_binom for each row of the column vector\n",
    "plt.plot(p, np.apply_along_axis(lambda guess: lh_binom(guess, sample), 1, p), 'r-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLE is derived as follows:\n",
    "$$ \\ln{Pr}(p;\\textbf{x}))=\\ln{p}\\sum_{i=1}^{n} x_i + \\ln{(1 - p)} \\sum_{i=1}^{n}(1-x_i) = \n",
    "k \\bar{\\textbf{x}} \\ln{p} + (n-k) \\bar{\\textbf{x}} \\ln{(1 - p)}$$\n",
    "Taking the derivative and setting to $0$ gives:\n",
    "$$ \\frac{k \\bar{\\textbf{x}}}{p} = \\frac{(n-k) \\bar{\\textbf{x}}}{1-p} $$\n",
    "$$ k \\bar{\\textbf{x}} (1-p) = (n-k) \\bar{\\textbf{x}} p $$\n",
    "$$ p = \\frac{k}{n} $$\n",
    "So $p$ is simply the sample mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.sum() / len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to get the MLE of p without doing the derivation, we need to find the p maximum of lh_binom(p, sample)\n",
    "minimize_scalar(lambda p: -1 * lh_binom(p, sample), method='bounded', bounds=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that:\n",
    "* in practice we have a closed form solution so #successess / #trials is faster, more accurate\n",
    "  and more numerically stable\n",
    "* in practice the loglikelihood is often maximized, since:\n",
    "  $$ \\prod f(x) \\text{ becomes } \\sum \\ln{f(x)}$$\n",
    "  and computing the derivatives and setting to $0$ becomes much easier!\n",
    "* the function value is -7.89e-31. Because we are multiplying p-values this procedure\n",
    "  will become numerically unstable when the sample size increases. We could apply the\n",
    "  log-sum-exp trick to make it numerically more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectaton-minimization algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The algorithm consist of iterating over two steps:\n",
    "0. start with an initial guess of the parameters\n",
    "1. using the current guess of the parameters fill compute the probability for each value of the missing data\n",
    "2. use these conditional probabilities to obtain better values of the parameters\n",
    "3. using the complete data = observed data + missing data estimate the parameters\n",
    "4. Iterate steps 2 and 3 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example: Assume we have two procedures with a different $Pr$ of success.<br>\n",
    "We observe a sample that is a mixture of these two procedures, but we do not know which procedure was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "ntrial = 50\n",
    "success_rate = {0: 0.4, ## success rate procedure 1\n",
    "                1: 0.7} ## success rate procedure 2\n",
    "def gen_data():\n",
    "    gr = np.random.choice(2, p=(0.45,0.55))\n",
    "    pr = success_rate[gr]\n",
    "    return(np.concatenate([np.array([gr]),statdist.bernoulli(p=pr).rvs(ntrial)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array([gen_data() for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = sample[:,0].reshape((-1,1))\n",
    "nrs = sample[:,1:].sum(axis=1).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([grp,nrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## expectation step: compute the expected value of the missing data given the parameters\n",
    "def e_step(est, observed_data):\n",
    "    return(np.argmax(np.hstack([statdist.binom(p=est[0], n=ntrial).pmf(observed_data),\n",
    "                                statdist.binom(p=est[1], n=ntrial).pmf(observed_data)\n",
    "                               ]), \n",
    "                     axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maximization step: compute param for which likelihood given observed & missing data\n",
    "def m_step(missing_data, observed_data):\n",
    "    return(np.array([np.mean(observed_data[missing_data==0])/ntrial,\n",
    "                     np.mean(observed_data[missing_data==1])/ntrial\n",
    "                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_data = nrs\n",
    "missing_data  = np.random.choice(2, size=N)         ## initialize to some random start values\n",
    "old_param_est = np.zeros(2)                         ## initialize to 0,0\n",
    "nr_iteration  = 0                                   ## count iterations\n",
    "new_param_est = m_step(missing_data, observed_data) ## a first e-step to get the iterations going\n",
    "print(f'iteration {nr_iteration}: {100*new_param_est[0]:<.5f}% & {100*new_param_est[1]:<.5f}%')\n",
    "while ((np.sum(np.abs(new_param_est-old_param_est)) > 1e-6) & (nr_iteration < 50)):\n",
    "    missing_data  = e_step(new_param_est, observed_data)\n",
    "    old_param_est = new_param_est \n",
    "    new_param_est = m_step(missing_data, observed_data)\n",
    "    print(f'iteration {nr_iteration}: {100*new_param_est[0]:<.5f}% & {100*new_param_est[1]:<.5f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(missing_data.flatten() == grp.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "## compute weighted sum of squares\n",
    "def wgt_ss(data, wgt, avg, den):\n",
    "    wd = np.sqrt(wgt)[:,np.newaxis] * (data - avg)\n",
    "    return(np.dot(wd.T,wd) / den)\n",
    "\n",
    "## compute matrix with in each column the multivariate normal pdf foor a given cluster mean & covariance given cluster weights\n",
    "def compute_pdfwmvn(data, cl_wgt, cl_avg, cl_cov):\n",
    "    return(np.vstack([cl_wgt[nc] * multivariate_normal.pdf(data, cl_avg[nc], cl_cov[nc]) \n",
    "                      for nc in range(len(cl_wgt))\n",
    "                     ]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm(data, nclasses, maxiter = 100, conv_eps = 1e-5):\n",
    "    ncases   = data.shape[0]\n",
    "    llh      = np.log(conv_eps) * ncases\n",
    "    llh_prev = llh - 2 * conv_eps ## stop if llh close to llh_prev(ious iteration)\n",
    "    itr      = 0\n",
    "    ## setting some random start values for class probability\n",
    "    cl_wgt   = np.random.uniform(0.2, 0.8, nclasses)\n",
    "    cl_wgt   = cl_wgt / cl_wgt.sum()\n",
    "    ## create a random class membership variable\n",
    "    cls_ind  = np.random.choice(nclasses, p=cl_wgt, size=data.shape[0])\n",
    "    ## taking a random sample of indices for each class\n",
    "    cl_avg   = np.asarray([np.mean(data[cls_ind==ix,:],0) for ix in range(nclasses)])\n",
    "    cl_cov   = np.asarray([np.cov( data[cls_ind==ix,:].T) for ix in range(nclasses)])\n",
    "    ## given this random class membership, what are the conditional probabilities\n",
    "    cond_pr  = compute_pdfwmvn(data, cl_wgt, cl_avg, cl_cov)\n",
    "    ## loop over the \n",
    "    while itr < maxiter and np.sum(np.abs(llh - llh_prev)) > conv_eps:\n",
    "        llh_prev  = llh\n",
    "        wgt       = cond_pr / cond_pr.sum(axis=1)[:,np.newaxis]\n",
    "        nk        = wgt.sum(axis=0)\n",
    "        cl_wgt    = nk / ncases\n",
    "        cl_avg    = np.dot(wgt.T, data) / nk.T[:,np.newaxis]\n",
    "        cl_cov    = np.asarray([wgt_ss(data, wgt[:,nc], cl_avg[nc,:], nk[nc]) \n",
    "                                for nc in range(nclasses)\n",
    "                               ])\n",
    "        cond_pr   = compute_pdfwmvn(data, cl_wgt, cl_avg, cl_cov)\n",
    "        llh       = np.sum(np.log(cond_pr.sum(axis=1)))\n",
    "        print(f'iter {itr}: avg class 1: {cl_avg[0]}, avg class 2 : {cl_avg[1]}')\n",
    "        itr += 1\n",
    "    return(cl_wgt, cl_avg, cl_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = np.random.multivariate_normal(mean=np.array([ 1, 1]), cov=np.diag([1,1]), size=50)\n",
    "dat2 = np.random.multivariate_normal(mean=np.array([-1,-1]), cov=np.diag([2,2]), size=25)\n",
    "data = np.vstack([dat1, dat2])\n",
    "data = data[np.random.permutation(np.arange(data.shape[0])),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, m, c = gmm(data, 2)\n",
    "print(f'cov class 1:\\n{c[0]}\\ncov class 2:\\n{c[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_wgt   = np.random.uniform(0.2, 0.8, 2)\n",
    "cl_wgt   = cl_wgt / cl_wgt.sum()\n",
    "cl_wgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_ind  = np.random.choice(2, p=cl_wgt, size=data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x - x.mean(axis=1)[:,np.newaxis]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
