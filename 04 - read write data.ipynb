{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read and write external files, there is no need to import any additional libraries.<br>\n",
    "Python contains a function **open()** in the standard library that will give you an object (\\_io.TextIOWrapper) to use when reading and writing data to/from a file.<br>\n",
    "In it's simplest form, to read a text file, it takes a single arguments: a filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open??\n",
    "## as an aside, you can import the signature function in the inspect module to see \n",
    "## from inspect import signature\n",
    "## signature(open)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the function returns a stream.<br>\n",
    "When we pass in a single argument, a filename, it opens a file for reading (more='r')\n",
    "<pre>\n",
    "========= ===============================================================\n",
    "Character Meaning\n",
    "--------- ---------------------------------------------------------------\n",
    "'r'       open for reading (default)\n",
    "'w'       open for writing, truncating the file first\n",
    "'x'       create a new file and open it for writing\n",
    "'a'       open for writing, appending to the end of the file if it exists\n",
    "'b'       binary mode\n",
    "'t'       text mode (default)\n",
    "'+'       open a disk file for updating (reading and writing)\n",
    "'U'       universal newline mode (deprecated)\n",
    "========= ===============================================================\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced remark**<br><br>\n",
    "The io module provides Pythonâ€™s main facilities for dealing with various types of I/O. There are three main types of I/O: text I/O, binary I/O and raw I/O.<br>\n",
    "These are generic categories, and various backing stores can be used for each of them. A concrete object belonging to any of these categories is called a file object.<br>\n",
    "Other common terms are stream and file-like object.<br>\n",
    "See: [io documentation](https://docs.python.org/3/library/io.html)<br><br>\n",
    "**Mortal usage**<br><br>\n",
    "To keep thing simple, to read data from a file (stream) use\n",
    "* **read()** to read the whole file in one go\n",
    "* **readlines()** to read the file line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open('data\\iris.data')\n",
    "## reads the complete file in one go\n",
    "content = fh.read()\n",
    "fh.close()\n",
    "content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is more usefull to read a file a line at a time and process that line.<br><br>\n",
    "Another tip is to use a so called context manager:\n",
    "<pre>\n",
    "with open(...) as f:\n",
    "    f.read...\n",
    "some new commands\n",
    "</pre>\n",
    "This automatically closes the file once the scope of the context manager ends.<br>\n",
    "So, combineing this we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "with open('data\\iris.data') as file:\n",
    "    ## read file line by line\n",
    "    for line in file:\n",
    "        content.append(line)\n",
    "## print the first 5 lines --> note the \\n still part of each line --> use strip() to remove whitespace and \\n at beginning / end of each line\n",
    "content[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## there is always a shorter way in Python, not nessesarily more readible --> create a list containing the first 10 lines and strip the \\n :\n",
    "[line.strip() for line_number, line in enumerate(open('data\\iris.data')) if line_number < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## or, how about a dictionary of the first 10 lines where the key is the line number\n",
    "{line_number: line.strip() for line_number, line in enumerate(open('data\\iris.data')) if line_number < 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, when you read in a file, you are likely to encounter some problems and want to catch possible errors.<br>\n",
    "So I find it usefull to wrap the processing that needs doing (usually splitting andf converting to the correct type) in a seperate function, say **process_line**.<br>\n",
    "This also enable to print the lines containing issues, but keep going fo the lines that are ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## lets read the file line by line and convert colun 1 to 4 to float\n",
    "## there is always a better way: context manager & iterating over the file handle stream\n",
    "list_of_records = []\n",
    "\n",
    "def process_line(ix, line):\n",
    "    if line == '':\n",
    "        print(f'warning: line {ix} was empty!')\n",
    "        return(None)\n",
    "    try:\n",
    "        e1, e2, e3, e4, e5 = line.split(',')\n",
    "        return(tuple([float(e1), float(e2), float(e3), float(e4), e5]))\n",
    "    except:\n",
    "        print(f'warning: line {ix} containing the string: {line.strip()}')\n",
    "        return(None)\n",
    "\n",
    "with open('data\\iris.data') as fh:\n",
    "    for ix, line in enumerate(fh):\n",
    "        tpl = process_line(ix, line.strip())\n",
    "        if tpl != None:\n",
    "            list_of_records.append(tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_records[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Database Connectivity (ODBC) is a standard application programming interface (API) for accessing database management systems (DBMS) . The designers of ODBC aimed to make it independent of database systems and operating systems.<br>\n",
    "Many databases provide ODBC clients. Using one of these ODBC clients, you can add an ODBC connection and name it, this name is known as the DSN (Data Source Name).<br>\n",
    "It should in principle be possible to open ODBC connection to SAS, Aster, Teradata, SQL Server, ... (I never managed to set up an ODBC connection to SAS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ODBC tool](plots/odbc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up a connection\n",
    "cnx = pyodbc.connect('dsn=NBSDataTD', autocommit=True)\n",
    "## from a connection we can request a cursor -> a cursor provides the 'context' of a fetch operation \n",
    "crs = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop test table if it already exists\n",
    "## crs.execute('drop table prl_013_cattag_v2.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the cursor provides the environment to delegate queries to the database and provide methods to access the table resulting from the query\n",
    "crs.execute('create table prl_013_cattag_v2.test ( id int, val float )')\n",
    "crs.execute('insert into prl_013_cattag_v2.test values (1, 1.1)')\n",
    "crs.execute('insert into prl_013_cattag_v2.test values (2, 2.2)')\n",
    "crs.execute('insert into prl_013_cattag_v2.test values (3, 3.3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a toy table:\n",
    "<pre>\n",
    "-- drop table prl_013_cattag_v2.test;\n",
    "create table prl_013_cattag_v2.test ( id int, val float );\n",
    "insert into prl_013_cattag_v2.test values (1, 1.1);\n",
    "insert into prl_013_cattag_v2.test values (2, 2.2);\n",
    "insert into prl_013_cattag_v2.test values (3, 3.3);\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when the query returns data the cursor can be used to iterate over the rows\n",
    "res = crs.execute('select * from prl_013_cattag_v2.test order by id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[method_name for method_name in dir(res) if callable(getattr(res, method_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rec in res:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to fetch one row from the cursor call fetchone\n",
    "crs.execute('select * from prl_013_cattag_v2.test order by id').fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to fetch all row from the cursor call fetchone\n",
    "crs.execute('select * from prl_013_cattag_v2.test order by id').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## to get to the schema of the table that resulted from the query run description\n",
    "crs.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs.execute('insert into prl_013_cattag_v2.test values (4, 66.99)')\n",
    "## when a query does not return a table --> description returns None\n",
    "crs.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading From Other Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the libraries that makes working with Python so much more user-friendly is pandas.<br>\n",
    "There will be a seperate notebook just on Pandas. Untill then, do not worry too much about the details.<br>\n",
    "Just note that Pandas, provides two data types: **Series** and **DataFrame**.<br>\n",
    "A Series captures a sequence of data indexed by an index, usually and integer, but could also be a date, string, ....<br>\n",
    "A DataFrame is a list of same-length sequences indexed by an index. It is the Python version of an R data.frame or an SQL table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of a SQL query is always a table<br>\n",
    "(this is what makes SQL so powerfull: input tables --> output tables, more formally closed under joining / merging / updating/ selecting / ...),<br>\n",
    "Pandas provides a convenient way to work with Databases, namely the **read_sql** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first import Pandas --> note everybody uses the alias pd for pandas!\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from prl_013_cattag_v2.test order by id', cnx)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can read from a multitude of data formats, for instance SAS<br>\n",
    "As an example, let's create a toy dataset in SAS (\\*.sas7bdat file)\n",
    "<pre>\n",
    "libname temp '...'\n",
    "data temp.sas_test;\n",
    "    infile datalines delimiter=','; \n",
    "    input name $ dob :ddmmyy8. tscore nmatches;\n",
    "\tformat dob date9.;\n",
    "\tavgscore = tscore / nmatches;\n",
    "    datalines;\n",
    "Lisa,01032011,211,3\n",
    "Bart,.,431,5\n",
    "Homer,21011993,510,9\n",
    "Marge,17101995,747,9\n",
    "Basil,17101995,.,12\n",
    ";\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note the encoding='utf-8' is needed!\n",
    "df = pd.read_sas(r'\\\\csdatg03\\tfs_code\\p413248\\discovery analytics\\library\\python\\tutorial\\data\\sas_test.sas7bdat', encoding='utf-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delimited File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\iris.data', header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "species_to_color_map = {'Iris-setosa'    : 'blue', 'Iris-versicolor': 'orange', 'Iris-virginica' : 'green'}\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(25,15))\n",
    "\n",
    "for x in range(4):\n",
    "    for y in range(4):\n",
    "        if x==y: \n",
    "            for grp, dfs in df.groupby('species'): \n",
    "                sns.distplot(dfs[dfs.columns[x]], ax=ax[x,y], hist=False, color=species_to_color_map[grp], label=grp)\n",
    "            ax[x,y].legend()\n",
    "        else:\n",
    "            sns.scatterplot(x=df.iloc[:,x], y=df.iloc[:,y], hue=df.iloc[:,4], ax=ax[x,y])\n",
    "\n",
    "## plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exchanging DataFrames Between Python & R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the most efficient way to exchange dataframes between Python and R is through **feather**.<br>\n",
    "Feather is build on top of the Apache Arrow project, an in memory columnar format envisaged to make sharing data between different tools seamless. See [apache arrow](https://arrow.apache.org/).<br>\n",
    "Feather is a fast on-disk version implemented for both [Python](https://github.com/wesm/feather) and [R](https://blog.rstudio.com/2016/03/29/feather/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather.write_dataframe(df,'toy_data.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R:<br>\n",
    "install.packages(\"feather\")<br>\n",
    "require(feather)<br>\n",
    "df <- read_feather(\"toy_data.feather\")<br>\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persisting Python Objects: pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from writing simple text data, or JSON, or XML, Python provides ways to persisting whole objects using the pickle library.<br>\n",
    "Meaning, the state and all of it's methods. Lets create a more complex object to persist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a dictionary of functions\n",
    "def my_add(a,b): return(a+b)\n",
    "def my_sub(a,b): return(a-b)\n",
    "def my_mul(a,b): return(a*b)\n",
    "def my_div(a,b): return(a/b)\n",
    "fncs = { '+': my_add, '-': my_sub, '*': my_mul, '/': my_div }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this can be used like:\n",
    "fncs['+'](1.1,22.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fncs, open('fncs.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_func_straight_from_a_file = pickle.load(open('fncs.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func_straight_from_a_file['+'](1.1,22.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!del *.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & Writing To/From non-local files: Sockets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an advanced encore, there are two files in the script directory that use sockets to pass data around.<br>\n",
    "Sockets are file abstractions where data is written and read -or in socket terminology- send and received, over a network.<br>\n",
    "When combined with pickling, it opens posibilities to communicate and share objects between different machines on a network.<br>\n",
    "Please see these files to explore further if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the files:\n",
    "* open two command windows in the script directory\n",
    "* run: _python server.py_ to start the server and\n",
    "* run: _python client.py_ to run the client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
